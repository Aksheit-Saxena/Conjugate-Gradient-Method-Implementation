# -*- coding: utf-8 -*-
"""Q3_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rb_RXvXtbKr2TblBsP1SLJZCb0I3ZtYm
"""

# Step 1: Coding up blackboxes for f, gradf, hessianf 
import numpy as np
def f(x):
    x1=x[0]
    x2=x[1]
    result=(10*(x1**4))-(20*(x1**2)*x2)+(10*(x2**2))-(2*x1)+5
    return result

def gradf(x):
    x1=x[0]
    x2=x[1]
    g1=(40*(x1**3))-(40*x1*x2)-2
    g2=(-20*(x1**2))+(20*x2)
    g=np.array([g1,g2])
    return g
    
def hessian(x):
    x1=x[0]
    x2=x[1]
    result_1=[(120*(x1**2)) -(40*x2),40*x1]
    result_2=[-40*x1,20]
    result=np.array([result_1,result_2])
    return result

def conjugate_descent(x0,e):
    converged = False
    trajec=[]
    x = x0
    trajec.append(x)
    iters = 0
    grad_f_norm=10000
    while iters<=1:
        g = gradf(x)
        # compute gradient
        if iters==0: 
          d=-g
          alpha=-g.T@d/(d.T@hessian(x)@d)
          x=x+alpha*(-g)
          trajec.append(x)
          grad_f_norm = np.linalg.norm(trajec[-1]-x0)
          
        else :
         beta=g.T@hessian(x)@d/(d.T@hessian(x)@d)
         d=-g+beta*d
         alpha=-g.T@d/(d.T@hessian(x)@d)
         x=x+alpha*(-g)
         trajec.append(x)
         grad_f_norm = np.linalg.norm(trajec[-1]-trajec[-2])

        iters += 1
        
    
    return [x,grad_f_norm]

x = np.array([-1.5,3]) 
e=0.01
dist=10000
i=0
while(dist>=e):
  i+=1
  x_opt = conjugate_descent(x,e)[0]
  dist=conjugate_descent(x,e)[1]
  print("Iteration",i)
  print("######")
  print("x: ", x_opt)
  print("convergence distance",dist)
  print("The function value at is : ",f(x))
  x=np.array(x_opt)

